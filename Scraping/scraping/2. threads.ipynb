{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:\n",
    "> - I dati sono sufficienti ma occorre ristrutturarli, in quanto diversi da come li vorremmo\n",
    "> - Le reply non sono dirette: il subject delle reply è \"Re: [subject originale]\" e il body è il testo della reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "TEMP_DIR = os.path.join(DATA_DIR, \"temp\")\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repos creation\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "os.makedirs(RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "TOP_GAMES_LIST_FILE = os.path.join(DATA_DIR, \"boardgames_ranks.csv\")\n",
    "USERNAMES_FILE = os.path.join(TEMP_DIR, \"usernames.json\")\n",
    "FORUMS_FILE = os.path.join(RAW_DIR, \"forums.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General download parameters\n",
    "BACKUP_PERIOD = 25 # Frequency of data backup\n",
    "REQUEST_DELAY = 0.01    # Delay between requests\n",
    "MAX_RETRIES = 5 # Max number of retries for a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = False    # If True, the existing files will be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game interval definition\n",
    "GAME_RANK_MIN = 1   # Rank of the first game to download\n",
    "GAME_RANK_MAX = 2000    # Rank of the last game to download\n",
    "GAME_NUM = GAME_RANK_MAX - GAME_RANK_MIN + 1    # Number of games to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific download parameters\n",
    "MAX_FORUMS_PER_GAME = 10    # Maximum number of forums to download for each game\n",
    "MIN_THREADS_PER_FORUM = 8   # Minimum number of threads to download for each forum\n",
    "MAX_THREADS_PER_FORUM = 20  # Maximum number of threads to download for each forum\n",
    "MIN_ARTICLES_PER_THREAD = 10    # Minimum number of articles to download for each thread\n",
    "MAX_ARTICLES_PER_THREAD = 70    # Maximum number of articles to download for each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "BGG_BASE_URL = \"https://boardgamegeek.com/xmlapi2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    \"\"\"\n",
    "    Save data to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    data (any): The data to be saved to the JSON file. This can be any data type that is serializable to JSON.\n",
    "    filename (str): The name of the file where the data will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_json(new_data, filename):\n",
    "    \"\"\"\n",
    "    Adds new data to an existing JSON file without duplicates.\n",
    "\n",
    "        new_data (list): The new data to add.\n",
    "        filename (str): The name of the JSON file.\n",
    "    \"\"\"\n",
    "    # Read existing data from the file.\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = []\n",
    "\n",
    "    # Combine the existing data with the new data and remove duplicates.\n",
    "    combined_data = {json.dumps(item, sort_keys=True): item for item in existing_data + new_data}.values()\n",
    "\n",
    "    # Save the combined data back to the file.\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(combined_data), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches the list of forums associated with a game ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_forum_list(game_id, max_forums=100, min_threads=5, sleep_time=0.5, max_retries=5):\n",
    "    \"\"\"\n",
    "        Fetches the list of forums associated with a game.\n",
    "        \n",
    "        Args:\n",
    "            game_id (int): The ID of the game for which to fetch the forums.\n",
    "            max_forums (int, optional): The maximum number of forums to fetch. Defaults to 100.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, each containing information about a forum:\n",
    "                - id (str): The ID of the forum.\n",
    "                - title (str): The title of the forum.\n",
    "                - num_threads (int): The number of threads in the forum.\n",
    "                - num_posts (int): The number of posts in the forum.\n",
    "                - last_post_date (str): The date of the last post in the forum.\n",
    "    \"\"\"\n",
    "    # Build the URL for requesting the forum list\n",
    "    url = f\"{BGG_BASE_URL}/forumlist?id={game_id}&type=thing\"\n",
    "    status_code = 500\n",
    "    \n",
    "    # Retry loop for fetching the threads from the forum\n",
    "    while status_code != 200:\n",
    "        # Pause briefly between retries\n",
    "        time.sleep(sleep_time)  # Delay\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            status_code = response.status_code\n",
    "            if status_code == 200:\n",
    "                break\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                # Log an error if retries are exhausted\n",
    "                logger.error(f\"Error fetching threads for forum {forum_id}: {status_code}. Retries exhausted.\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            # Log an error if an exception occurs\n",
    "            logger.error(f\"Error fetching threads for forum {forum_id}: {e}.\")\n",
    "            return []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML response\n",
    "        root = ET.fromstring(response.content)\n",
    "        forums = []\n",
    "        # Iterate over each forum in the XML, respecting the max_forums limit\n",
    "        for forum in root.findall(\"forum\")[:max_forums]:\n",
    "            # Capture relevant forum information in a dictionary\n",
    "            if forum.attrib.get(\"numthreads\") is not None and int(forum.attrib.get(\"numthreads\")) > 5:\n",
    "                forums.append({\n",
    "                    \"id\": forum.attrib.get(\"id\"),\n",
    "                    \"title\": forum.attrib.get(\"title\"),\n",
    "                    \"num_threads\": int(forum.attrib.get(\"numthreads\", 0)),\n",
    "                    \"num_posts\": int(forum.attrib.get(\"posts\", 0)),\n",
    "                    \"last_post_date\": forum.attrib.get(\"lastpostdate\")\n",
    "                })\n",
    "        # Return the collected forum data\n",
    "        return forums\n",
    "    else:\n",
    "        # Log an error if the request was unsuccessful\n",
    "        logger.error(f\"Error fetching forums for game {game_id}: {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches the list of threads associated with a forum ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_threads_from_forum(forum_id, max_threads=5, min_articles=10, max_articles=100, sleep_time=0.5, max_retries=5):\n",
    "    \"\"\"\n",
    "        Fetches threads from a specific forum, limiting the number of threads.\n",
    "        \n",
    "        Args:\n",
    "            forum_id (int): The ID of the forum to fetch threads from.\n",
    "            max_threads (int, optional): The maximum number of threads to fetch. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries, each containing details of a thread. Each dictionary has the following keys:\n",
    "                - thread_id (str): The ID of the thread.\n",
    "                - author (str): The author of the thread.\n",
    "                - subject (str): The subject of the thread.\n",
    "                - num_articles (int): The number of articles in the thread.\n",
    "                - post_date (str): The post date of the thread.\n",
    "                - last_post_date (str): The last post date of the thread.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{BGG_BASE_URL}/forum?id={forum_id}\"\n",
    "    status_code = 500\n",
    "    \n",
    "    # Retry loop for fetching the threads from the forum\n",
    "    while status_code != 200:\n",
    "        # Pause briefly between retries\n",
    "        time.sleep(sleep_time)  # Delay\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            status_code = response.status_code\n",
    "            if status_code == 200:\n",
    "                break\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                # Log an error if retries are exhausted\n",
    "                logger.error(f\"Error fetching threads for forum {forum_id}: {status_code}. Retries exhausted.\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            # Log an error if an exception occurs\n",
    "            logger.error(f\"Error fetching threads for forum {forum_id}: {e}.\")\n",
    "            return []\n",
    "    \n",
    "    if status_code == 200:\n",
    "        # Parse the XML response\n",
    "        root = ET.fromstring(response.content)\n",
    "        threads = []\n",
    "        # Iterate over each thread in the XML, respecting the max_threads limit\n",
    "        for thread in root.findall(\"threads/thread\")[:max_threads]:\n",
    "            # Capture relevant thread information in a dictionary\n",
    "            if thread.attrib.get(\"numarticles\") is not None and int(thread.attrib.get(\"numarticles\")) > min_articles and int(thread.attrib.get(\"numarticles\")) < max_articles:\n",
    "                threads.append({\n",
    "                    \"thread_id\": thread.attrib.get(\"id\"),\n",
    "                    \"author\": thread.attrib.get(\"author\"),\n",
    "                    \"subject\": thread.attrib.get(\"subject\"),\n",
    "                    \"num_articles\": int(thread.attrib.get(\"numarticles\", 0)),\n",
    "                    \"post_date\": thread.attrib.get(\"postdate\"),\n",
    "                    \"last_post_date\": thread.attrib.get(\"lastpostdate\")\n",
    "                })\n",
    "        # Return the collected thread data\n",
    "        return threads\n",
    "    else:\n",
    "        # Log an error if the request was unsuccessful\n",
    "        logger.error(f\"Error fetching threads for forum {forum_id}: {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches the list of messages associated with a thread ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_messages_from_thread(thread_id, max_posts=5, sleep_time=0.5, max_retries=5):\n",
    "    \"\"\"\n",
    "        Fetches messages from a specific thread, supporting pagination.\n",
    "        Args:\n",
    "            thread_id (int): The ID of the thread to fetch messages from.\n",
    "            max_posts (int, optional): The maximum number of posts to fetch. Defaults to 5.\n",
    "            sleep_time (float, optional): The time to sleep between retries in seconds. Defaults to 0.5.\n",
    "            max_retries (int, optional): The maximum number of retries for fetching messages. Defaults to 5.\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - messages (list): A list of dictionaries, each containing details of a message.\n",
    "                - usernames (list): A list of unique usernames who posted in the thread.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    usernames = set()\n",
    "    \n",
    "    # Build the URL for requesting the thread messages\n",
    "    url = f\"{BGG_BASE_URL}/thread?id={thread_id}\"\n",
    "    status_code = 500\n",
    "\n",
    "    # Retry loop for fetching the thread messages\n",
    "    while status_code != 200:\n",
    "        # Pause briefly between retries\n",
    "        time.sleep(sleep_time)  # Delay\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            status_code = response.status_code\n",
    "            if status_code == 200:\n",
    "                break\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                # Log an error if retries are exhausted\n",
    "                logger.error(f\"Error fetching messages for thread {thread_id}: {status_code}. Retries exhausted.\")\n",
    "                return [], []\n",
    "        except Exception as e:\n",
    "            # Log an error if an exception occurs\n",
    "            logger.error(f\"Error fetching messages for thread {thread_id}: {e}.\")\n",
    "            return [], []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML response\n",
    "        root = ET.fromstring(response.content)\n",
    "        # Iterate over each article in the XML, respecting the max_posts limit\n",
    "        for article in root.find(\"articles\").findall(\"article\")[:max_posts]:\n",
    "            usernames.add(article.attrib.get(\"username\"))\n",
    "            # Capture relevant message information in a dictionary\n",
    "            messages.append({\n",
    "                \"article_id\": article.attrib.get(\"id\"),\n",
    "                \"username\": article.attrib.get(\"username\"),\n",
    "                \"post_date\": article.attrib.get(\"postdate\"),\n",
    "                \"edit_date\": article.attrib.get(\"editdate\"),\n",
    "                \"num_edits\": int(article.attrib.get(\"numedits\", 0)),\n",
    "                \"subject\": article.find(\"subject\").text if article.find(\"subject\") is not None else None,\n",
    "                \"content\": article.find(\"body\").text if article.find(\"body\") is not None else None\n",
    "            })\n",
    "        logger.info(f\"\\t\\tDownloaded {len(messages)} messages from thread {thread_id}\")\n",
    "    else:\n",
    "        # Log an error if the request was unsuccessful\n",
    "        logger.error(f\"Error fetching messages for thread {thread_id}: {response.status_code}\")\n",
    "        return [], []\n",
    "    \n",
    "    # Return the collected messages and unique usernames\n",
    "    return messages, list(usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves the game list from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(USERNAMES_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    usernames = set(json.load(f))\n",
    "    \n",
    "# Top games data\n",
    "top_games_df = pd.read_csv(TOP_GAMES_LIST_FILE)\n",
    "games = top_games_df.loc[GAME_RANK_MIN-1:GAME_RANK_MAX-1, ['id', 'name']]\n",
    "\n",
    "# Remove games already collected\n",
    "if not OVERWRITE:\n",
    "    try:\n",
    "        with open(FORUMS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            collected_ids = [game[\"id\"] for game in data]\n",
    "            games = [game for game in games if game['id'] not in collected_ids]\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "logger.info(f\"Games to collect: {len(games)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downoald threads content for each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_once = not OVERWRITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching forums:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching forums: 100%|██████████| 1/1 [00:34<00:00, 34.17s/it]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "i = 0\n",
    "for name, game_id in tqdm(zip(games[\"name\"], games[\"id\"]), desc=\"Fetching forums\", total=games.shape[0]):\n",
    "    if logger.isEnabledFor(logging.INFO):\n",
    "        print()\n",
    "    game = {\n",
    "        \"id\": game_id,\n",
    "        \"name\": name,\n",
    "        \"forums\": []\n",
    "    }\n",
    "    # Fetch the list of forums for the specified game\n",
    "    forums = fetch_forum_list(game_id, max_forums=MAX_FORUMS_PER_GAME, min_threads=MIN_THREADS_PER_FORUM, sleep_time=REQUEST_DELAY, max_retries=MAX_RETRIES)\n",
    "    logger.info(f\"Retrieving {len(forums)} forums for game {game['name']}...\")\n",
    "    \n",
    "    # For each forum in the list of forums\n",
    "    for forum in forums:\n",
    "        # Fetch the threads from the specified forum\n",
    "        threads = fetch_threads_from_forum(forum[\"id\"], max_threads=MAX_THREADS_PER_FORUM, min_articles=MIN_ARTICLES_PER_THREAD, max_articles=MAX_ARTICLES_PER_THREAD, sleep_time=REQUEST_DELAY, max_retries=MAX_RETRIES)\n",
    "        logger.info(f\"\\tRetrieving {len(threads)} threads for forum {forum['title']}...\")\n",
    "\n",
    "        # For each thread in the list of threads\n",
    "        for thread in threads:\n",
    "            # Fetch the messages and users from the specified thread\n",
    "            thread[\"messages\"], users = fetch_messages_from_thread(thread[\"thread_id\"], max_posts=MAX_ARTICLES_PER_THREAD, sleep_time=REQUEST_DELAY, max_retries=MAX_RETRIES)\n",
    "            # Update the set of usernames with the fetched users\n",
    "            usernames.update(users)\n",
    "\n",
    "        # Add the threads to the forum\n",
    "        forum[\"threads\"] = threads\n",
    "\n",
    "    # Add the forums to the game\n",
    "    game[\"forums\"] = forums\n",
    "\n",
    "    # Append the game data to the list of all data\n",
    "    all_data.append(game)\n",
    "\n",
    "    i += 1\n",
    "    if i % BACKUP_PERIOD == 0:\n",
    "        # Save the data to the JSON file\n",
    "        if not saved_once:\n",
    "            # Save the data to the JSON file\n",
    "            save_to_json(all_data, FORUMS_FILE)\n",
    "            saved_once = True\n",
    "        else:\n",
    "            # Append the data to the JSON file\n",
    "            append_to_json(all_data, FORUMS_FILE)\n",
    "        logger.info(f\"Saved {i} games into '{FORUMS_FILE}'\")\n",
    "\n",
    "        # Reset the data list\n",
    "        all_data = []\n",
    "    \n",
    "# Save the remaining data to the JSON file\n",
    "if logger.isEnabledFor(logging.INFO):\n",
    "    print()\n",
    "if all_data:\n",
    "    if not saved_once:\n",
    "        # Save the data to the JSON file\n",
    "        save_to_json(all_data, FORUMS_FILE)\n",
    "    else:\n",
    "        # Append the data to the JSON file\n",
    "        append_to_json(all_data, FORUMS_FILE)\n",
    "    logger.info(f\"Saved remaining {len(all_data)} games forums into '{FORUMS_FILE}'\")\n",
    "\n",
    "# Save the set of usernames to the JSON file\n",
    "save_to_json(list(usernames), USERNAMES_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
