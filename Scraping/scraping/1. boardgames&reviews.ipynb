{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Retrieve top games and reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nei dati scaricati mancano: \n",
    "> - date delle reviews\n",
    "> - location della review\n",
    "> - gli id degli utenti che hanno scritto le reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositories\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "TEMP_DIR = os.path.join(DATA_DIR, \"temp\")\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repos creation\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "os.makedirs(RAW_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "TOP_GAMES_LIST_FILE = os.path.join(DATA_DIR, \"boardgames_ranks.csv\")\n",
    "USERNAMES_FILE = os.path.join(TEMP_DIR, \"usernames.json\")\n",
    "BOARD_GAMES_FILE = os.path.join(RAW_DIR, \"boardgames&reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General download parameters\n",
    "BACKUP_PERIOD = 200 # Frequency of data backup\n",
    "REQUEST_DELAY = 0.01    # Delay between requests\n",
    "MAX_RETRIES = 5 # Max number of retries for a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = False    # If True, the existing files will be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game interval definition\n",
    "GAME_RANK_MIN = 1   # Rank of the first game to download\n",
    "GAME_RANK_MAX = 2000    # Rank of the last game to download\n",
    "GAME_NUM = GAME_RANK_MAX - GAME_RANK_MIN + 1    # Number of games to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific download parameters\n",
    "REVIEWS_PER_GAME_LIMIT = 500    # Maximum number of reviews to download for each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "BGG_BASE_URL = \"https://boardgamegeek.com/xmlapi2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    \"\"\"\n",
    "    Save data to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    data (any): The data to be saved to the JSON file. This can be any data type that is serializable to JSON.\n",
    "    filename (str): The name of the file where the data will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_json(new_data, filename):\n",
    "    \"\"\"\n",
    "    Adds new data to an existing JSON file without duplicates.\n",
    "\n",
    "        new_data (list): The new data to add.\n",
    "        filename (str): The name of the JSON file.\n",
    "    \"\"\"\n",
    "    # Read existing data from the file.\n",
    "    try:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = []\n",
    "\n",
    "    # Combine the existing data with the new data and remove duplicates.\n",
    "    combined_data = {json.dumps(item, sort_keys=True): item for item in existing_data + new_data}.values()\n",
    "\n",
    "    # Save the combined data back to the file.\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(combined_data), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Board game details extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_game_data(xml_root):\n",
    "    \"\"\"\n",
    "    Parses the XML data of board games and extracts relevant information.\n",
    "\n",
    "    Args:\n",
    "        xml_root (Element): The root element of the XML tree containing game data.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing information about a game.\n",
    "    \"\"\"\n",
    "    games = []\n",
    "    for item in xml_root.findall(\"item\"):\n",
    "        game = {\n",
    "            \"id\": item.attrib.get(\"id\"),  \n",
    "            \"name\": item.find(\"name\").attrib.get(\"value\"),\n",
    "            \"description\": item.find(\"description\").text if item.find(\"description\") is not None else \"\", \n",
    "            \"imageURL\": item.find(\"image\").text if item.find(\"image\") is not None else \"\", \n",
    "            \"rating\": 0.0,  # Inserted after fetching reviews\n",
    "            \"yearReleased\": int(item.find(\"yearpublished\").attrib.get(\"value\", 0)),\n",
    "            \"minPlayers\": int(item.find(\"minplayers\").attrib.get(\"value\", 0)),\n",
    "            \"maxPlayers\": int(item.find(\"maxplayers\").attrib.get(\"value\", 0)),\n",
    "            \"minSuggAge\": int(item.find(\"minage\").attrib.get(\"value\", 0)),\n",
    "            \"minPlayTime\": int(item.find(\"minplaytime\").attrib.get(\"value\", 0)),\n",
    "            \"maxPlayTime\": int(item.find(\"maxplaytime\").attrib.get(\"value\", 0)),\n",
    "            \"designers\": [link.attrib.get(\"value\") for link in item.findall(\"link[@type='boardgamedesigner']\")],\n",
    "            \"artists\": [link.attrib.get(\"value\") for link in item.findall(\"link[@type='boardgameartist']\")],\n",
    "            \"publishers\": [link.attrib.get(\"value\") for link in item.findall(\"link[@type='boardgamepublisher']\")],\n",
    "            \"categories\": [link.attrib.get(\"value\") for link in item.findall(\"link[@type='boardgamecategory']\")],\n",
    "            \"mechanisms\": [link.attrib.get(\"value\") for link in item.findall(\"link[@type='boardgamemechanic']\")],\n",
    "            \"family\": [link.attrib.get(\"value\") for link in item.findall(\"link[@type='boardgamefamily']\")],\n",
    "            \"reviews\": []  # Populated by fetch_reviews\n",
    "        }\n",
    "        games.append(game)\n",
    "    return games\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description is not included in the XML file, so we need to scrape directly BGG website to retrieve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_short_description(game_id, sleep_time=1, max_retries=5):\n",
    "    \"\"\"\n",
    "    Fetches the short description of a board game from BoardGameGeek.\n",
    "    Args:\n",
    "        game_id (int): The ID of the board game to fetch the description for.\n",
    "        sleep_time (int, optional): The time to sleep between retries in seconds (default is 1).\n",
    "        max_retries (int, optional): The maximum number of retries for the request (default is 5).\n",
    "    Returns:\n",
    "        str: The short description of the board game if found.\n",
    "        tuple: An empty tuple if the description is not found or if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the API endpoint URL for fetching reviews.\n",
    "    url = f\"https://boardgamegeek.com/boardgame/{game_id}\"\n",
    "    \n",
    "    # Define headers for the request to simulate a real browser and avoid being blocked.\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Initialize the status code to simulate a retry loop until the response is successful.\n",
    "    status_code = 500\n",
    "\n",
    "    # Retry loop: Keeps trying until a successful response is received or retries are exhausted.\n",
    "    while status_code != 200:\n",
    "        time.sleep(sleep_time)  # Add a delay between retries to avoid overloading the server.\n",
    "        try:\n",
    "            # Make a GET request to the URL with the headers.\n",
    "            response = requests.get(url, headers=headers)\n",
    "            status_code = response.status_code  # Update the status code with the response.\n",
    "            \n",
    "            # Break the loop if the response is successful (status code 200).\n",
    "            if status_code == 200:\n",
    "                break\n",
    "            \n",
    "            # Decrement retries left and handle the case when no retries remain.\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                logger.error(f\"Error during retrieving short description for ID {game_id}: {status_code}. No retries left.\")\n",
    "                return None  # Return empty results if retries are exhausted.\n",
    "        \n",
    "        # Handle network errors or exceptions during the request.\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during retrieving short description for ID {game_id}: {e}\")\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                logger.error(f\"Error during retrieving short description for ID {game_id}. No retries left.\")\n",
    "                return None # Return empty results if retries are exhausted.\n",
    "\n",
    "    # Check again for a successful response after exiting the loop.\n",
    "    if response.status_code == 200: # Parse the HTML content        \n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the short description in the HTML content.\n",
    "        meta_tag = soup.find('meta', {'name': 'description'})\n",
    "        if meta_tag and 'content' in meta_tag.attrs:\n",
    "            return (meta_tag['content'])\n",
    "        else:\n",
    "            logger.warning(f\"Short description not found for game ID {game_id}.\")\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        # Handle unexpected cases where the response status is not successful.\n",
    "        logger.error(f\"Error during retrieving short description for ID {game_id}: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review extraction from a given board game review XML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reviews(game_id, page=1, sleep_time=1, max_retries=5):\n",
    "    \"\"\"\n",
    "    Fetches reviews for a specific game from BoardGameGeek.\n",
    "\n",
    "    Args:\n",
    "        game_id (int): The ID of the game to fetch reviews for.\n",
    "        page (int, optional): The page number of reviews to fetch. Defaults to 1.\n",
    "        sleep_time (int, optional): The time to wait between retries in seconds. Defaults to 1.\n",
    "        max_retries (int, optional): The maximum number of retries in case of failure. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - reviews (list of dict): A list of dictionaries, each containing 'user', 'rating', and 'comment' keys.\n",
    "            - usernames (list of str): A list of unique usernames who have reviewed the game.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the API endpoint URL for fetching reviews.\n",
    "    url = f\"{BGG_BASE_URL}/thing?id={game_id}&comments=1&page={page}&pagesize=100\"\n",
    "    \n",
    "    # Define headers for the request to simulate a real browser and avoid being blocked.\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Initialize the status code to simulate a retry loop until the response is successful.\n",
    "    status_code = 500\n",
    "\n",
    "    # Retry loop: Keeps trying until a successful response is received or retries are exhausted.\n",
    "    while status_code != 200:\n",
    "        time.sleep(sleep_time)  # Add a delay between retries to avoid overloading the server.\n",
    "        try:\n",
    "            # Make a GET request to the URL with the headers.\n",
    "            response = requests.get(url, headers=headers)\n",
    "            status_code = response.status_code  # Update the status code with the response.\n",
    "            \n",
    "            # Break the loop if the response is successful (status code 200).\n",
    "            if status_code == 200:\n",
    "                break\n",
    "            \n",
    "            # Decrement retries left and handle the case when no retries remain.\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                logger.error(f\"Error during retrieving reviews for ID {game_id}: {status_code}. No retries left.\")\n",
    "                return [], []  # Return empty results if retries are exhausted.\n",
    "        \n",
    "        # Handle network errors or exceptions during the request.\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during retrieving reviews for ID {game_id}: {e}\")\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                logger.error(f\"Error during retrieving reviews for ID {game_id}. No retries left.\")\n",
    "                return [], []  # Return empty results if retries are exhausted.\n",
    "\n",
    "    # Check again for a successful response after exiting the loop.\n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML response content.\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Initialize storage for reviews and unique usernames.\n",
    "        reviews = []\n",
    "        usernames = set()\n",
    "\n",
    "        # Extract individual comments from the XML.\n",
    "        for comment in root.findall(\"item/comments/comment\"):\n",
    "            username = comment.attrib.get(\"username\")  # Get the username of the commenter.\n",
    "            usernames.add(username)  # Add the username to the set to ensure uniqueness.\n",
    "            \n",
    "            # Append the parsed review to the reviews list.\n",
    "            reviews.append({\n",
    "                \"user\": username,\n",
    "                \"rating\": float(comment.attrib.get(\"rating\", 0)) if comment.attrib.get(\"rating\", \"N/A\") != \"N/A\" else \"N/A\",\n",
    "                \"comment\": comment.attrib.get(\"value\", \"\").strip()\n",
    "            })\n",
    "        \n",
    "        # Return the list of reviews and the unique usernames as a tuple.\n",
    "        return reviews, list(usernames)\n",
    "    else:\n",
    "        # Handle unexpected cases where the response status is not successful.\n",
    "        logger.error(f\"Error during retrieving reviews for ID {game_id}: {response.status_code}\")\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate previous functions to retrieve all data for a given board game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_game_data(game_id, review_limit=np.inf, sleep_time=0.5, max_retries=5):\n",
    "    \"\"\"\n",
    "    Fetches game data and includes reviews from BoardGameGeek.\n",
    "\n",
    "    Parameters:\n",
    "    game_id (int): The ID of the game to fetch data for.\n",
    "    review_limit (int, optional): The maximum number of reviews to fetch. Defaults to np.inf.\n",
    "    sleep_time (float, optional): The time to wait between requests in seconds. Defaults to 0.5.\n",
    "    max_retries (int, optional): The maximum number of retries for failed requests. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - game_data (list): A list with the game data dictionary.\n",
    "        - usernames (list): A list of unique usernames who reviewed the game.\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"{BGG_BASE_URL}/thing?id={game_id}&stats=1\"\n",
    "    status_code = 500\n",
    "\n",
    "    while status_code != 200:\n",
    "        time.sleep(sleep_time)  # Delay\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            status_code = response.status_code\n",
    "            if status_code == 200:\n",
    "                break\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                logger.error(f\"Error for ID {game_id}: {status_code}. No retries left.\")\n",
    "                return None, []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error for ID {game_id}: {e}\")\n",
    "            max_retries -= 1\n",
    "            if max_retries == 0:\n",
    "                logger.error(f\"Error for ID {game_id}. No retries left.\")\n",
    "                return None, []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Fetch game data\n",
    "        root = ET.fromstring(response.content)\n",
    "        game_data = parse_game_data(root)\n",
    "        rating = 0.0\n",
    "        n_reviews = 0\n",
    "        # Fetch short description\n",
    "        short_description = fetch_short_description(game_id)\n",
    "        if short_description:\n",
    "            game_data[0][\"short_description\"] = short_description\n",
    "        else:\n",
    "            game_data[0][\"short_description\"] = \"\"\n",
    "        # Fetch reviews\n",
    "        if game_data:\n",
    "            time.sleep(sleep_time)   # 1s delay\n",
    "            reviews = []\n",
    "            usernames = set()\n",
    "            page = 1\n",
    "            while len(reviews) < review_limit:\n",
    "                new_reviews, users = fetch_reviews(game_id, page)\n",
    "                if not new_reviews:\n",
    "                    break\n",
    "                reviews.extend(new_reviews)\n",
    "                usernames.update(users)\n",
    "                rating += sum(r[\"rating\"] for r in new_reviews if r[\"rating\"] != \"N/A\")\n",
    "                n_reviews += len([r for r in new_reviews if r[\"rating\"] != \"N/A\"])\n",
    "                page += 1\n",
    "                time.sleep(REQUEST_DELAY)\n",
    "            game_data[0][\"reviews\"] = reviews\n",
    "        # Add rating\n",
    "        game_data[0][\"rating\"] = rating / n_reviews if n_reviews > 0 else 0.0\n",
    "        logger.info(f\"Game ID {game_id}: {game_data[0]['name']} - {len(reviews)} reviews\")\n",
    "        return game_data, list(usernames)\n",
    "    else:\n",
    "        logger.error(f\"Error for ID {game_id}: {response.status_code}\")\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve top games list from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top games data\n",
    "top_games_df = pd.read_csv(TOP_GAMES_LIST_FILE)\n",
    "game_ids = top_games_df['id'].tolist()[GAME_RANK_MIN-1:GAME_RANK_MAX]\n",
    "\n",
    "# Remove games already collected\n",
    "if not OVERWRITE:\n",
    "    try:\n",
    "        with open(BOARD_GAME_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            collected_ids = [game[\"id\"] for game in data]\n",
    "            game_ids = [game_id for game_id in game_ids if game_id not in collected_ids]\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "logger.info(f\"Games to collect: {len(game_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the XML page for each game and extract the data, saving it periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_once = not OVERWRITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Boardgames download: 100%|██████████| 1/1 [00:12<00:00, 12.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "all_games = []\n",
    "all_users = set()\n",
    "\n",
    "logger.info(\"BoardGames data scraping...\")\n",
    "for i, game_id in enumerate(tqdm(game_ids, desc=\"Boardgames download\"), start=1):\n",
    "    if logger.isEnabledFor(logging.INFO):\n",
    "        print()\n",
    "    game_data, users = fetch_game_data(game_id, review_limit=REVIEWS_PER_GAME_LIMIT)\n",
    "    if game_data:\n",
    "        all_games.extend(game_data)\n",
    "        all_users.update(users)\n",
    "\n",
    "    if i % BACKUP_PERIOD == 0:\n",
    "        if not saved_once:\n",
    "            save_to_json(all_games, BOARD_GAMES_FILE)\n",
    "            saved_once = True\n",
    "        else:\n",
    "            append_to_json(all_games, BOARD_GAMES_FILE)\n",
    "        if logger.isEnabledFor(logging.INFO):\n",
    "            print()\n",
    "        logger.info(f\"Saved {i} games into '{BOARD_GAMES_FILE}'\")\n",
    "\n",
    "        # Reset variable\n",
    "        all_games = []\n",
    "\n",
    "    time.sleep(REQUEST_DELAY)  # Delay\n",
    "\n",
    "# Save any remaining games\n",
    "if logger.isEnabledFor(logging.INFO):\n",
    "    print()\n",
    "if all_games:\n",
    "    if not saved_once:\n",
    "        save_to_json(all_games, BOARD_GAMES_FILE)\n",
    "        saved_once = True\n",
    "    else:\n",
    "        append_to_json(all_games, BOARD_GAMES_FILE)\n",
    "    logger.info(f\"Saved remaining {len(all_games)} games into '{BOARD_GAMES_FILE}'\")\n",
    "\n",
    "# Save usernames\n",
    "save_to_json(list(all_users), USERNAMES_FILE)\n",
    "logger.info(f\"Saved {len(all_users)} users into '{USERNAMES_FILE}'\")\n",
    "\n",
    "logger.info(f\"BoardGames data scraping completed. Data saved into '{BOARD_GAMES_FILE}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
